{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODS XML Analysis Notebook\n",
    "\n",
    "This notebook retrieves and analyzes MODS XML metadata from an API endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import random\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import openpyxl\n",
    "from openpyxl.styles import Font, Alignment, PatternFill\n",
    "from openpyxl.utils import get_column_letter\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from itertools import islice\n",
    "import math\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "XML_DIR = \"mods_records\"  # Directory containing XML files\n",
    "\n",
    "#SAMPLE=100\n",
    "SAMPLE=None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_logging():\n",
    "    \"\"\"Configure logging with both file and console handlers.\"\"\"\n",
    "    # Create logs directory if it doesn't exist\n",
    "    os.makedirs('logs', exist_ok=True)\n",
    "    \n",
    "    # Create formatters for different levels of detail\n",
    "    brief_formatter = logging.Formatter('%(message)s')\n",
    "    verbose_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    \n",
    "    # Create and configure handlers\n",
    "    log_filename = f\"logs/mods_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"\n",
    "    \n",
    "    # Clear any existing handlers\n",
    "    logger = logging.getLogger()\n",
    "    for handler in logger.handlers[:]:\n",
    "        logger.removeHandler(handler)\n",
    "        \n",
    "    # File handler - gets everything with full detail\n",
    "    file_handler = logging.FileHandler(log_filename)\n",
    "    file_handler.setLevel(logging.DEBUG)\n",
    "    file_handler.setFormatter(verbose_formatter)\n",
    "    \n",
    "    # Console handler - gets just the important stuff briefly\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setLevel(logging.INFO)\n",
    "    console_handler.setFormatter(brief_formatter)\n",
    "    \n",
    "    # Configure root logger\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    logger.addHandler(file_handler)\n",
    "    logger.addHandler(console_handler)\n",
    "    \n",
    "    return logger\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing Functions\n",
    "\n",
    "For every record, extract the specific data you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_publisher(record):\n",
    "    \"\"\"Extract publisher information from MODS record.\"\"\"\n",
    "    publisher = record.find(\".//{*}publisher\")\n",
    "    if publisher is not None and publisher.text:\n",
    "        return publisher.text.strip()\n",
    "    return None\n",
    "\n",
    "def extract_classifications(record):\n",
    "    \"\"\"Extract classification numbers from MODS record.\"\"\"\n",
    "    classifications = {\n",
    "        'lcc': None,\n",
    "        'ddc': None,\n",
    "        'other': []  # For any other classification schemes\n",
    "    }\n",
    "    \n",
    "    for classification in record.findall(\".//{*}classification\"):\n",
    "        if classification.text:\n",
    "            authority = classification.get('authority', '').lower()\n",
    "            value = classification.text.strip()\n",
    "            \n",
    "            if authority == 'lcc':\n",
    "                classifications['lcc'] = value\n",
    "            elif authority == 'ddc':\n",
    "                classifications['ddc'] = value\n",
    "            else:\n",
    "                # Store other classification schemes with their authority\n",
    "                classifications['other'].append({\n",
    "                    'authority': authority,\n",
    "                    'value': value\n",
    "                })\n",
    "    \n",
    "    return classifications\n",
    "\n",
    "def extract_identifiers(record):\n",
    "    \"\"\"Extract standard identifiers from MODS record, excluding local identifiers.\"\"\"\n",
    "    identifiers = {\n",
    "        'issn': None,\n",
    "        'issn_l': None, \n",
    "        'lccn': None,\n",
    "        'oclc': None,\n",
    "        'doi': None,\n",
    "        'isbn': None,\n",
    "        'stock_number': None,\n",
    "        'issn_other_online': None,\n",
    "    }\n",
    "    \n",
    "    # Extract standard identifiers\n",
    "    for identifier in record.findall(\".//{*}identifier\"):\n",
    "        id_type = identifier.get('type', '')\n",
    "        \n",
    "        # Skip local identifiers\n",
    "        if id_type == 'local':\n",
    "            continue\n",
    "            \n",
    "        # Skip invalid identifiers\n",
    "        if identifier.get('invalid') == 'yes':\n",
    "            continue\n",
    "            \n",
    "        if identifier.text:\n",
    "            value = identifier.text.strip()\n",
    "            \n",
    "            if id_type == 'issn':\n",
    "                identifiers['issn'] = value\n",
    "            elif id_type == 'issn-l':\n",
    "                identifiers['issn_l'] = value\n",
    "            elif id_type == 'lccn':\n",
    "                identifiers['lccn'] = value\n",
    "            elif id_type == 'oclc':\n",
    "                identifiers['oclc'] = value\n",
    "            elif id_type == 'doi':\n",
    "                identifiers['doi'] = value\n",
    "            elif id_type == 'isbn':\n",
    "                identifiers['isbn'] = value\n",
    "            elif id_type == 'stock number':\n",
    "                identifiers['stock_number'] = value\n",
    "    \n",
    "    # Look for online version ISSN in relatedItems\n",
    "    for related_item in record.findall(\".//{*}relatedItem\"):\n",
    "        # Check if this is an online version\n",
    "        is_online = False\n",
    "        display_label = related_item.get('displayLabel', '').lower()\n",
    "        other_type = related_item.get('otherType', '').lower()\n",
    "        item_type = related_item.get('type', '').lower()\n",
    "        \n",
    "        if ('online' in display_label or \n",
    "            'online' in other_type or \n",
    "            'online' in item_type):\n",
    "            is_online = True\n",
    "        \n",
    "        if is_online:\n",
    "            # Look for ISSN within this relatedItem\n",
    "            for identifier in related_item.findall(\".//{*}identifier[@type='issn']\"):\n",
    "                if identifier.text:\n",
    "                    identifiers['issn_other_online'] = identifier.text.strip()\n",
    "                    break  # Take the first one we find\n",
    "                \n",
    "    return identifiers\n",
    "\n",
    "    \n",
    "def extract_genres(record):\n",
    "    \"\"\"Extract genre information from both direct genre tags and subject/genre elements,\n",
    "    standardizing singular/plural forms.\"\"\"\n",
    "    genres = set()\n",
    "    \n",
    "    # Helper function to standardize genre text\n",
    "    def standardize_genre(genre_text):\n",
    "        genre_text = re.sub(r'\\.+$', '', genre_text.strip().lower())\n",
    "        \n",
    "        if genre_text.lower() == \"periodical\":\n",
    "            return \"periodicals\"\n",
    "        return genre_text.lower()\n",
    "    \n",
    "    # Get genres from direct genre tags\n",
    "    for genre in record.findall(\".//{*}genre\"):\n",
    "        if genre is not None and genre.text:\n",
    "            genres.add(standardize_genre(genre.text))\n",
    "    \n",
    "    # Get genres from subject/genre elements\n",
    "    for subject in record.findall(\".//{*}subject\"):\n",
    "        for genre in subject.findall(\".//{*}genre\"):\n",
    "            if genre is not None and genre.text:\n",
    "                genres.add(standardize_genre(genre.text))\n",
    "    \n",
    "    return sorted(set(genres))\n",
    "\n",
    "def extract_start_date(origin_info):\n",
    "    \"\"\"Extract start date from originInfo element.\"\"\"\n",
    "    date_issued = origin_info.find(\".//{*}dateIssued[@encoding='marc'][@point='start']\")\n",
    "    if date_issued is not None and date_issued.text:\n",
    "        try:\n",
    "            return int(date_issued.text)\n",
    "        except ValueError:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def extract_end_date(origin_info):\n",
    "    \"\"\"Extract start date from originInfo element.\"\"\"\n",
    "    date_issued = origin_info.find(\".//{*}dateIssued[@encoding='marc'][@point='end']\")\n",
    "    if date_issued is not None and date_issued.text:\n",
    "        try:\n",
    "            return int(date_issued.text)\n",
    "        except ValueError:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def extract_lcsh_subjects(record):\n",
    "    \"\"\"Extract subject topics from LCSH.\"\"\"\n",
    "    subjects = set()\n",
    "    for subject in record.findall(\".//{*}subject[@authority='lcsh']\"):\n",
    "        topics = subject.findall(\".//{*}topic\")\n",
    "        for topic in topics:\n",
    "            if topic is not None and topic.text:\n",
    "                subjects.add(topic.text.lower())\n",
    "    return list(subjects)\n",
    "\n",
    "def extract_fast_subjects(record):\n",
    "    \"\"\"Extract subject topics from LCSH.\"\"\"\n",
    "    subjects = set()\n",
    "    for subject in record.findall(\".//{*}subject[@authority='fast']\"):\n",
    "        topics = subject.findall(\".//{*}topic\")\n",
    "        for topic in topics:\n",
    "            if topic is not None and topic.text:\n",
    "                subjects.add(topic.text.lower())\n",
    "    return list(subjects)\n",
    "\n",
    "def extract_record_identifier(record):\n",
    "    \"\"\"Extract ALMA identifier from MODS record with fallbacks.\"\"\"\n",
    "    try:\n",
    "        # Try to get ALMA identifier\n",
    "        record_info = record.find(\".//{*}recordInfo/{*}recordIdentifier[@source='MH:ALMA']\")\n",
    "        if record_info is not None and record_info.text:\n",
    "            return record_info.text\n",
    "        \n",
    "        # First fallback: any record identifier\n",
    "        record_info = record.find(\".//{*}recordInfo/{*}recordIdentifier\")\n",
    "        if record_info is not None and record_info.text:\n",
    "            return f\"record_{record_info.text}\"\n",
    "        \n",
    "        # Second fallback: title\n",
    "        title_info = record.find(\".//{*}titleInfo/{*}title\")\n",
    "        if title_info is not None and title_info.text:\n",
    "            return sanitize_filename(title_info.text)\n",
    "            \n",
    "        # Final fallback\n",
    "        return \"untitled_record\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting identifier: {e}\")\n",
    "        return \"error_record\"\n",
    "\n",
    "def determine_format(record):\n",
    "    \"\"\"Determine if the resource is print, electronic, microfilm, microfiche, or online.\"\"\"\n",
    "    formats = set()\n",
    "    \n",
    "    # Check physical description\n",
    "    phys_desc = record.find(\".//{*}physicalDescription\")\n",
    "    if phys_desc is not None:\n",
    "        formats.add('probably print')\n",
    "        # Check all form elements\n",
    "        for form in phys_desc.findall(\".//{*}form\"):\n",
    "            if form is not None and form.text:\n",
    "                formats.add(form.text.lower())\n",
    "\n",
    "    # Check for microform information in reproduction notes\n",
    "    for note in record.findall(\".//{*}note[@type='reproduction']\"):\n",
    "        if note is not None and note.text:\n",
    "            note_text = note.text.lower()\n",
    "            if 'microfilm' in note_text:\n",
    "                formats.add('microfilm')\n",
    "            if 'microfiche' in note_text:\n",
    "                formats.add('microfiche')\n",
    "    \n",
    "    # Check for microfilm in relatedItems\n",
    "    for related_item in record.findall(\".//{*}relatedItem\"):\n",
    "        # Check various attributes that might indicate microfilm version\n",
    "        display_label = related_item.get('displayLabel', '').lower()\n",
    "        other_type = related_item.get('otherType', '').lower()\n",
    "        item_type = related_item.get('type', '').lower()\n",
    "        \n",
    "        # Also check the title within relatedItem\n",
    "        title_element = related_item.find(\".//{*}title\")\n",
    "        title_text = title_element.text.lower() if title_element is not None and title_element.text else ''\n",
    "        \n",
    "        # If any of these mention microfilm, add it to formats\n",
    "        if any('microfilm' in x for x in [display_label, other_type, item_type, title_text]):\n",
    "            formats.add('microfilm')\n",
    "    \n",
    "    # Check for any online-related information in relatedItems\n",
    "    for related_item in record.findall(\".//{*}relatedItem\"):\n",
    "        # Check various attributes that might indicate online version\n",
    "        display_label = related_item.get('displayLabel', '').lower()\n",
    "        other_type = related_item.get('otherType', '').lower()\n",
    "        item_type = related_item.get('type', '').lower()\n",
    "        \n",
    "        if any('online' in x for x in [display_label, other_type, item_type]):\n",
    "            formats.add(\"online\")\n",
    "            break\n",
    "\n",
    "    # NEW: Check for specific genre values that indicate online format\n",
    "    for genre in record.findall(\".//{*}genre\"):\n",
    "        if genre is not None and genre.text:\n",
    "            genre_text = genre.text.strip().lower()\n",
    "            if (genre_text == \"computer network resources.\" or \n",
    "                genre_text == \"electronic journals.\"):\n",
    "                formats.add(\"online\")\n",
    "                break\n",
    "    \n",
    "    return list(formats) if formats else [\"unknown\"]\n",
    "\n",
    "def determine_resource_type(record):\n",
    "    \"\"\"Determine if the resource is a journal/serial or book.\"\"\"\n",
    "    origin_info = record.find(\".//{*}originInfo\")\n",
    "    if origin_info is not None:\n",
    "        issuance = origin_info.find(\".//{*}issuance\")\n",
    "        if issuance is not None and issuance.text:\n",
    "            return issuance.text.strip()  # Return the text value instead of Element\n",
    "    return \"unknown\"\n",
    "\n",
    "def extract_locations(record):\n",
    "    \"\"\"Extract physical locations and Harvard repositories from MODS record.\"\"\"\n",
    "    locations = set()\n",
    "    \n",
    "    # Get physical locations\n",
    "    for location in record.findall(\".//{*}physicalLocation\"):\n",
    "        if location is not None and location.text:\n",
    "            locations.add(location.text.strip())\n",
    "    \n",
    "    # Get Harvard repositories from extension\n",
    "    for repository in record.findall(\".//{*}HarvardRepository\"):\n",
    "        if repository is not None and repository.text:\n",
    "            locations.add(repository.text.strip())\n",
    "    \n",
    "    # Return as sorted list\n",
    "    return sorted(list(locations)) if locations else []\n",
    "\n",
    "def extract_subject_authorities(record):\n",
    "    \"\"\"Extract unique subject authorities from MODS record.\"\"\"\n",
    "    authorities = set()\n",
    "    \n",
    "    for subject in record.findall(\".//{*}subject\"):\n",
    "        authority = subject.get('authority')\n",
    "        if authority:  # Only add if authority exists\n",
    "            authorities.add(authority.lower())  # Normalize to lowercase\n",
    "    \n",
    "    return sorted(list(authorities))  # Return as sorted list\n",
    "\n",
    "def extract_place(record):\n",
    "    \"\"\"Extract place information from MODS record, preferring text version.\"\"\"\n",
    "    origin_info = record.find(\".//{*}originInfo\")\n",
    "    if origin_info is not None:\n",
    "        # First try to find a placeTerm with type=\"text\"\n",
    "        for place in origin_info.findall(\".//{*}placeTerm[@type='text']\"):\n",
    "            if place is not None and place.text:\n",
    "                return place.text.strip()\n",
    "    \n",
    "    return None\n",
    "\n",
    "def extract_language(record):\n",
    "    \"\"\"Extract language information from MODS record, preferring text version.\"\"\"\n",
    "    language_element = record.find(\".//{*}language\")\n",
    "    if language_element is not None:\n",
    "        # Look for languageTerm with type=\"text\"\n",
    "        language_term = language_element.find(\".//{*}languageTerm[@type='text']\")\n",
    "        if language_term is not None and language_term.text:\n",
    "            return language_term.text.strip()\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data from XML Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xml_files(directory, sample_size=None):\n",
    "    \"\"\"\n",
    "    Get list of XML files from directory, optionally sampling random subset.\n",
    "    \n",
    "    Args:\n",
    "        directory (str): Path to directory containing XML files\n",
    "        sample_size (int, optional): Number of files to randomly sample. If None, use all files.\n",
    "    \n",
    "    Returns:\n",
    "        list: Paths to XML files to process\n",
    "    \"\"\"\n",
    "    # Get all XML files in directory\n",
    "    xml_files = list(Path(directory).glob('*.xml'))\n",
    "    logger.info(f\"Found {len(xml_files)} XML files in {directory}\")\n",
    "    \n",
    "    if not xml_files:\n",
    "        logger.error(f\"No XML files found in {directory}\")\n",
    "        return []\n",
    "    \n",
    "    # Sample if requested and possible\n",
    "    if sample_size and sample_size < len(xml_files):\n",
    "        xml_files = random.sample(xml_files, sample_size)\n",
    "        logger.info(f\"Randomly sampled {sample_size} files for processing\")\n",
    "    \n",
    "    return xml_files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the XML files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_xml_files(xml_files, chunk_size=100000):\n",
    "    \"\"\"\n",
    "    Process XML files in chunks and save intermediate results.\n",
    "    \n",
    "    Args:\n",
    "        xml_files (list): List of paths to XML files\n",
    "        chunk_size (int): Number of files to process before saving intermediate results\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame: Combined data from all processed files\n",
    "    \"\"\"\n",
    "    all_publications = []\n",
    "    total_files = len(xml_files)\n",
    "    chunk_number = 1\n",
    "    \n",
    "    # Process files in chunks\n",
    "    for chunk in chunk_files(xml_files, chunk_size):\n",
    "        chunk_publications = []\n",
    "        chunk_start = (chunk_number - 1) * chunk_size\n",
    "        \n",
    "        # Use ThreadPoolExecutor for parallel processing of files within chunk\n",
    "        with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "            futures = []\n",
    "            for xml_file in chunk:\n",
    "                future = executor.submit(process_single_file, xml_file)\n",
    "                futures.append(future)\n",
    "            \n",
    "            # Process results as they complete\n",
    "            for i, future in enumerate(futures):\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    if result:\n",
    "                        chunk_publications.append(result)\n",
    "                    \n",
    "                    # Log progress within chunk\n",
    "                    current_file = chunk_start + i + 1\n",
    "                    if current_file % 10000 == 0 or current_file == total_files:\n",
    "                        logger.info(f\"Processed {current_file}/{total_files} files\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error processing file: {str(e)}\")\n",
    "        \n",
    "        # Create DataFrame for chunk and save intermediate result\n",
    "        if chunk_publications:\n",
    "            chunk_df = pd.DataFrame(chunk_publications)\n",
    "            intermediate_file = f\"mods_analysis_chunk_{chunk_number}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx\"\n",
    "            save_formatted_excel(chunk_df, intermediate_file)\n",
    "            logger.info(f\"Saved intermediate chunk {chunk_number} to {intermediate_file}\")\n",
    "        \n",
    "        # Add chunk results to overall results\n",
    "        all_publications.extend(chunk_publications)\n",
    "        chunk_number += 1\n",
    "    \n",
    "    # Create final DataFrame from all processed records\n",
    "    df = pd.DataFrame(all_publications)\n",
    "    logger.info(f\"Created final DataFrame with {len(df)} records from {total_files} files\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process a Single Record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_file(xml_file):\n",
    "    \"\"\"Process a single XML file and return publication data.\"\"\"\n",
    "    try:\n",
    "        # Read and parse XML file\n",
    "        tree = ET.parse(xml_file)\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        # Process the record\n",
    "        publication_data = process_single_record(root, str(xml_file.name))\n",
    "        logger.debug(f\"Successfully processed {xml_file}\")\n",
    "        return publication_data\n",
    "        \n",
    "    except ET.ParseError as e:\n",
    "        logger.error(f\"Failed to parse {xml_file}: {str(e)}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing {xml_file}: {str(e)}\")\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_record(record, filename):\n",
    "    \"\"\"Process a single MODS record and return structured data.\"\"\"\n",
    "    try:\n",
    "        # Extract basic information\n",
    "        title_info = record.find(\".//{*}titleInfo/{*}title\")\n",
    "        title = title_info.text if title_info is not None else \"Unknown Title\"\n",
    "        \n",
    "        origin_info = record.find(\".//{*}originInfo\")\n",
    "        \n",
    "        # Use existing extraction functions\n",
    "        publication_data = {\n",
    "            'filename': filename,\n",
    "            'title': title,\n",
    "            'publisher': extract_publisher(record),\n",
    "            'start_year': extract_start_date(origin_info) if origin_info is not None else None,\n",
    "            'end_year': extract_end_date(origin_info) if origin_info is not None else None,\n",
    "            'format': determine_format(record),\n",
    "            'type': determine_resource_type(record),\n",
    "            'lcsh_subjects': extract_lcsh_subjects(record),\n",
    "            'fast_subjects': extract_fast_subjects(record),\n",
    "            'genres': extract_genres(record),\n",
    "            'locations': extract_locations(record),\n",
    "            'authorities': extract_subject_authorities(record),\n",
    "            'place': extract_place(record),  # Add new place field\n",
    "            'language': extract_language(record),  # Add new language field\n",
    "            **extract_identifiers(record),\n",
    "            'record_identifier': extract_record_identifier(record),\n",
    "            **extract_classifications(record)\n",
    "        }\n",
    "        \n",
    "        return publication_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing record: {str(e)}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Break the giant list of files up into smaller chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_files(files, chunk_size=100000):\n",
    "    \"\"\"Generator to split files into chunks.\"\"\"\n",
    "    iterator = iter(files)\n",
    "    return iter(lambda: list(islice(iterator, chunk_size)), [])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format the output XLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_formatted_excel(df, output_file):\n",
    "    \"\"\"Save DataFrame to Excel with consistent formatting.\"\"\"\n",
    "    # First save with pandas\n",
    "    df.to_excel(output_file, index=False)\n",
    "    \n",
    "    # Then open with openpyxl for formatting\n",
    "    wb = openpyxl.load_workbook(output_file)\n",
    "    ws = wb.active\n",
    "    \n",
    "    # Define styles\n",
    "    regular_font = Font(size=10)\n",
    "    header_font = Font(size=10, bold=True)\n",
    "    alignment = Alignment(wrap_text=True, vertical='top')\n",
    "    header_fill = PatternFill(start_color='E0E0E0', end_color='E0E0E0', fill_type='solid')\n",
    "    \n",
    "    # Format header row\n",
    "    for cell in ws[1]:\n",
    "        cell.font = header_font\n",
    "        cell.alignment = alignment\n",
    "        cell.fill = header_fill\n",
    "    \n",
    "    # Format data rows\n",
    "    for row in ws.iter_rows(min_row=2):  # Start from second row\n",
    "        for cell in row:\n",
    "            cell.font = regular_font\n",
    "            cell.alignment = alignment\n",
    "    \n",
    "    # Set all column widths to 12\n",
    "    for column in ws.columns:\n",
    "        column_letter = get_column_letter(column[0].column)\n",
    "        ws.column_dimensions[column_letter].width = 12\n",
    "    \n",
    "    # Freeze the header row\n",
    "    ws.freeze_panes = 'A2'\n",
    "    \n",
    "    # Add filters to header row\n",
    "    ws.auto_filter.ref = ws.dimensions\n",
    "    \n",
    "    # Save the formatted workbook\n",
    "    wb.save(output_file)\n",
    "    logger.info(f\"\\nSaved formatted analysis results to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting MODS analysis on directory: mods_records\n",
      "Found 885565 XML files in mods_records\n",
      "Estimated processing time: 24.6 hours\n",
      "Processed 10000/885565 files\n",
      "Processed 20000/885565 files\n",
      "Processed 30000/885565 files\n",
      "Processed 40000/885565 files\n",
      "Processed 50000/885565 files\n",
      "Processed 60000/885565 files\n",
      "Processed 70000/885565 files\n",
      "Processed 80000/885565 files\n",
      "Processed 90000/885565 files\n",
      "Processed 100000/885565 files\n",
      "\n",
      "Saved formatted analysis results to mods_analysis_chunk_1_20250214_135356.xlsx\n",
      "Saved intermediate chunk 1 to mods_analysis_chunk_1_20250214_135356.xlsx\n",
      "Processed 110000/885565 files\n",
      "Processed 120000/885565 files\n",
      "Processed 130000/885565 files\n",
      "Processed 140000/885565 files\n",
      "Processed 150000/885565 files\n",
      "Processed 160000/885565 files\n",
      "Processed 170000/885565 files\n",
      "Processed 180000/885565 files\n",
      "Processed 190000/885565 files\n",
      "Processed 200000/885565 files\n",
      "\n",
      "Saved formatted analysis results to mods_analysis_chunk_2_20250214_135722.xlsx\n",
      "Saved intermediate chunk 2 to mods_analysis_chunk_2_20250214_135722.xlsx\n",
      "Processed 210000/885565 files\n",
      "Processed 220000/885565 files\n",
      "Processed 230000/885565 files\n",
      "Processed 240000/885565 files\n",
      "Processed 250000/885565 files\n",
      "Processed 260000/885565 files\n",
      "Processed 270000/885565 files\n",
      "Processed 280000/885565 files\n",
      "Processed 290000/885565 files\n",
      "Processed 300000/885565 files\n",
      "\n",
      "Saved formatted analysis results to mods_analysis_chunk_3_20250214_140039.xlsx\n",
      "Saved intermediate chunk 3 to mods_analysis_chunk_3_20250214_140039.xlsx\n",
      "Processed 310000/885565 files\n",
      "Processed 320000/885565 files\n",
      "Processed 330000/885565 files\n",
      "Processed 340000/885565 files\n",
      "Processed 350000/885565 files\n",
      "Processed 360000/885565 files\n",
      "Processed 370000/885565 files\n",
      "Processed 380000/885565 files\n",
      "Processed 390000/885565 files\n",
      "Processed 400000/885565 files\n",
      "\n",
      "Saved formatted analysis results to mods_analysis_chunk_4_20250214_140408.xlsx\n",
      "Saved intermediate chunk 4 to mods_analysis_chunk_4_20250214_140408.xlsx\n",
      "Processed 410000/885565 files\n",
      "Processed 420000/885565 files\n",
      "Processed 430000/885565 files\n",
      "Processed 440000/885565 files\n",
      "Processed 450000/885565 files\n",
      "Processed 460000/885565 files\n",
      "Processed 470000/885565 files\n",
      "Processed 480000/885565 files\n",
      "Processed 490000/885565 files\n",
      "Processed 500000/885565 files\n",
      "\n",
      "Saved formatted analysis results to mods_analysis_chunk_5_20250214_140728.xlsx\n",
      "Saved intermediate chunk 5 to mods_analysis_chunk_5_20250214_140728.xlsx\n",
      "Processed 510000/885565 files\n",
      "Processed 520000/885565 files\n",
      "Processed 530000/885565 files\n",
      "Processed 540000/885565 files\n",
      "Processed 550000/885565 files\n",
      "Processed 560000/885565 files\n",
      "Processed 570000/885565 files\n",
      "Processed 580000/885565 files\n",
      "Processed 590000/885565 files\n",
      "Processed 600000/885565 files\n",
      "\n",
      "Saved formatted analysis results to mods_analysis_chunk_6_20250214_141056.xlsx\n",
      "Saved intermediate chunk 6 to mods_analysis_chunk_6_20250214_141056.xlsx\n",
      "Processed 610000/885565 files\n",
      "Processed 620000/885565 files\n",
      "Processed 630000/885565 files\n",
      "Processed 640000/885565 files\n",
      "Processed 650000/885565 files\n",
      "Processed 660000/885565 files\n",
      "Processed 670000/885565 files\n",
      "Processed 680000/885565 files\n",
      "Processed 690000/885565 files\n",
      "Processed 700000/885565 files\n",
      "\n",
      "Saved formatted analysis results to mods_analysis_chunk_7_20250214_141419.xlsx\n",
      "Saved intermediate chunk 7 to mods_analysis_chunk_7_20250214_141419.xlsx\n",
      "Processed 710000/885565 files\n",
      "Processed 720000/885565 files\n",
      "Processed 730000/885565 files\n",
      "Processed 740000/885565 files\n",
      "Processed 750000/885565 files\n",
      "Processed 760000/885565 files\n",
      "Processed 770000/885565 files\n",
      "Processed 780000/885565 files\n",
      "Processed 790000/885565 files\n",
      "Processed 800000/885565 files\n",
      "\n",
      "Saved formatted analysis results to mods_analysis_chunk_8_20250214_141731.xlsx\n",
      "Saved intermediate chunk 8 to mods_analysis_chunk_8_20250214_141731.xlsx\n",
      "Processed 810000/885565 files\n",
      "Processed 820000/885565 files\n",
      "Processed 830000/885565 files\n",
      "Processed 840000/885565 files\n",
      "Processed 850000/885565 files\n",
      "Processed 860000/885565 files\n",
      "Processed 870000/885565 files\n",
      "Processed 880000/885565 files\n",
      "Processed 885565/885565 files\n",
      "\n",
      "Saved formatted analysis results to mods_analysis_chunk_9_20250214_142040.xlsx\n",
      "Saved intermediate chunk 9 to mods_analysis_chunk_9_20250214_142040.xlsx\n",
      "Created final DataFrame with 885565 records from 885565 files\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'mods_analysis_final_20250214_142225.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Save final results\u001b[39;00m\n\u001b[0;32m     19\u001b[0m output_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmods_analysis_final_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 20\u001b[0m save_formatted_excel(df, output_file)\n\u001b[0;32m     21\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved final analysis results to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[9], line 40\u001b[0m, in \u001b[0;36msave_formatted_excel\u001b[1;34m(df, output_file)\u001b[0m\n\u001b[0;32m     37\u001b[0m ws\u001b[38;5;241m.\u001b[39mauto_filter\u001b[38;5;241m.\u001b[39mref \u001b[38;5;241m=\u001b[39m ws\u001b[38;5;241m.\u001b[39mdimensions\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Save the formatted workbook\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m wb\u001b[38;5;241m.\u001b[39msave(output_file)\n\u001b[0;32m     41\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSaved formatted analysis results to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\openpyxl\\workbook\\workbook.py:386\u001b[0m, in \u001b[0;36mWorkbook.save\u001b[1;34m(self, filename)\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite_only \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworksheets:\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_sheet()\n\u001b[1;32m--> 386\u001b[0m save_workbook(\u001b[38;5;28mself\u001b[39m, filename)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\openpyxl\\writer\\excel.py:291\u001b[0m, in \u001b[0;36msave_workbook\u001b[1;34m(workbook, filename)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_workbook\u001b[39m(workbook, filename):\n\u001b[0;32m    280\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Save the given workbook on the filesystem under the name filename.\u001b[39;00m\n\u001b[0;32m    281\u001b[0m \n\u001b[0;32m    282\u001b[0m \u001b[38;5;124;03m    :param workbook: the workbook to save\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    289\u001b[0m \n\u001b[0;32m    290\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 291\u001b[0m     archive \u001b[38;5;241m=\u001b[39m ZipFile(filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, ZIP_DEFLATED, allowZip64\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    292\u001b[0m     workbook\u001b[38;5;241m.\u001b[39mproperties\u001b[38;5;241m.\u001b[39mmodified \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow(tz\u001b[38;5;241m=\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mtimezone\u001b[38;5;241m.\u001b[39mutc)\u001b[38;5;241m.\u001b[39mreplace(tzinfo\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    293\u001b[0m     writer \u001b[38;5;241m=\u001b[39m ExcelWriter(workbook, archive)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\zipfile\\__init__.py:1331\u001b[0m, in \u001b[0;36mZipFile.__init__\u001b[1;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps, metadata_encoding)\u001b[0m\n\u001b[0;32m   1329\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m   1330\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1331\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mopen(file, filemode)\n\u001b[0;32m   1332\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m   1333\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m filemode \u001b[38;5;129;01min\u001b[39;00m modeDict:\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'mods_analysis_final_20250214_142225.xlsx'"
     ]
    }
   ],
   "source": [
    "global logger\n",
    "logger = setup_logging()\n",
    "\n",
    "logger.info(f\"Starting MODS analysis on directory: {XML_DIR}\")\n",
    "    \n",
    "# Get XML files to process\n",
    "xml_files = get_xml_files(XML_DIR, SAMPLE)\n",
    "if not xml_files:\n",
    "    logger.error(\"No XML files found\")\n",
    "\n",
    "# Calculate expected processing time (assuming ~0.1s per file)\n",
    "estimated_hours = len(xml_files) * 0.1 / 3600\n",
    "logger.info(f\"Estimated processing time: {estimated_hours:.1f} hours\")\n",
    "\n",
    "# Process files and create DataFrame\n",
    "df = process_xml_files(xml_files)\n",
    "\n",
    "# Save final results\n",
    "output_file = f\"mods_analysis_final_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx\"\n",
    "save_formatted_excel(df, output_file)\n",
    "logger.info(f\"Saved final analysis results to {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

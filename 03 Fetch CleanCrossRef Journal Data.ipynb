{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3483c24f-346a-4fd2-b819-21475042cfbc",
   "metadata": {},
   "source": [
    "# Fetch Clean CrossRef Journal Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36f99cae-c5c4-459d-a6b1-b07df5945e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Reading CSV file...\n",
      "INFO:__main__:Extracting ISSNs...\n",
      "INFO:__main__:Found 180223 unique ISSNs\n",
      "INFO:__main__:Processed 1000 issn\n",
      "INFO:__main__:Processed 2000 issn\n",
      "INFO:__main__:Processed 3000 issn\n",
      "INFO:__main__:Processed 4000 issn\n",
      "INFO:__main__:Processed 5000 issn\n",
      "INFO:__main__:Processed 6000 issn\n",
      "INFO:__main__:Processed 7000 issn\n",
      "INFO:__main__:Processed 8000 issn\n",
      "INFO:__main__:Processed 9000 issn\n",
      "INFO:__main__:Processed 10000 issn\n",
      "INFO:__main__:Processed 11000 issn\n",
      "INFO:__main__:Processed 12000 issn\n",
      "INFO:__main__:Processed 13000 issn\n",
      "INFO:__main__:Processed 14000 issn\n",
      "INFO:__main__:Processed 15000 issn\n",
      "INFO:__main__:Processed 16000 issn\n",
      "INFO:__main__:Processed 17000 issn\n",
      "INFO:__main__:Processed 18000 issn\n",
      "INFO:__main__:Processed 19000 issn\n",
      "INFO:__main__:Processed 20000 issn\n",
      "INFO:__main__:Processed 21000 issn\n",
      "INFO:__main__:Processed 22000 issn\n",
      "INFO:__main__:Processed 23000 issn\n",
      "INFO:__main__:Processed 24000 issn\n",
      "INFO:__main__:Processed 25000 issn\n",
      "INFO:__main__:Processed 26000 issn\n",
      "INFO:__main__:Processed 27000 issn\n",
      "INFO:__main__:Processed 28000 issn\n",
      "INFO:__main__:Processed 29000 issn\n",
      "INFO:__main__:Processed 30000 issn\n",
      "INFO:__main__:Processed 31000 issn\n",
      "INFO:__main__:Processed 32000 issn\n",
      "INFO:__main__:Processed 33000 issn\n",
      "INFO:__main__:Processed 34000 issn\n",
      "INFO:__main__:Processed 35000 issn\n",
      "INFO:__main__:Processed 36000 issn\n",
      "INFO:__main__:Processed 37000 issn\n",
      "INFO:__main__:Processed 38000 issn\n",
      "INFO:__main__:Processed 39000 issn\n",
      "INFO:__main__:Processed 40000 issn\n",
      "INFO:__main__:Processed 41000 issn\n",
      "INFO:__main__:Processed 42000 issn\n",
      "INFO:__main__:Processed 43000 issn\n",
      "INFO:__main__:Processed 44000 issn\n",
      "INFO:__main__:Processed 45000 issn\n",
      "INFO:__main__:Processed 46000 issn\n",
      "INFO:__main__:Processed 47000 issn\n",
      "INFO:__main__:Processed 48000 issn\n",
      "INFO:__main__:Processed 49000 issn\n",
      "INFO:__main__:Processed 50000 issn\n",
      "INFO:__main__:Processed 51000 issn\n",
      "INFO:__main__:Processed 52000 issn\n",
      "INFO:__main__:Processed 53000 issn\n",
      "INFO:__main__:Processed 54000 issn\n",
      "INFO:__main__:Processed 55000 issn\n",
      "INFO:__main__:Processed 56000 issn\n",
      "INFO:__main__:Processed 57000 issn\n",
      "INFO:__main__:Processed 58000 issn\n",
      "INFO:__main__:Processed 59000 issn\n",
      "INFO:__main__:Processed 60000 issn\n",
      "INFO:__main__:Processed 61000 issn\n",
      "INFO:__main__:Processed 62000 issn\n",
      "INFO:__main__:Processed 63000 issn\n",
      "INFO:__main__:Processed 64000 issn\n",
      "INFO:__main__:Processed 65000 issn\n",
      "INFO:__main__:Processed 66000 issn\n",
      "INFO:__main__:Processed 67000 issn\n",
      "INFO:__main__:Processed 68000 issn\n",
      "INFO:__main__:Processed 69000 issn\n",
      "INFO:__main__:Processed 70000 issn\n",
      "INFO:__main__:Processed 71000 issn\n",
      "INFO:__main__:Processed 72000 issn\n",
      "INFO:__main__:Processed 73000 issn\n",
      "INFO:__main__:Processed 74000 issn\n",
      "INFO:__main__:Processed 75000 issn\n",
      "INFO:__main__:Processed 76000 issn\n",
      "INFO:__main__:Processed 77000 issn\n",
      "INFO:__main__:Processed 78000 issn\n",
      "INFO:__main__:Processed 79000 issn\n",
      "INFO:__main__:Processed 80000 issn\n",
      "INFO:__main__:Processed 81000 issn\n",
      "INFO:__main__:Processed 82000 issn\n",
      "INFO:__main__:Processed 83000 issn\n",
      "INFO:__main__:Processed 84000 issn\n",
      "INFO:__main__:Processed 85000 issn\n",
      "INFO:__main__:Processed 86000 issn\n",
      "INFO:__main__:Processed 87000 issn\n",
      "INFO:__main__:Processed 88000 issn\n",
      "INFO:__main__:Processed 89000 issn\n",
      "INFO:__main__:Processed 90000 issn\n",
      "INFO:__main__:Processed 91000 issn\n",
      "INFO:__main__:Processed 92000 issn\n",
      "INFO:__main__:Processed 93000 issn\n",
      "INFO:__main__:Processed 94000 issn\n",
      "INFO:__main__:Processed 95000 issn\n",
      "INFO:__main__:Processed 96000 issn\n",
      "INFO:__main__:Processed 97000 issn\n",
      "INFO:__main__:Processed 98000 issn\n",
      "INFO:__main__:Processed 99000 issn\n",
      "INFO:__main__:Processed 100000 issn\n",
      "INFO:__main__:Processed 101000 issn\n",
      "INFO:__main__:Processed 102000 issn\n",
      "INFO:__main__:Processed 103000 issn\n",
      "INFO:__main__:Processed 104000 issn\n",
      "INFO:__main__:Processed 105000 issn\n",
      "INFO:__main__:Processed 106000 issn\n",
      "INFO:__main__:Processed 107000 issn\n",
      "INFO:__main__:Processed 108000 issn\n",
      "INFO:__main__:Processed 109000 issn\n",
      "INFO:__main__:Processed 110000 issn\n",
      "INFO:__main__:Processed 111000 issn\n",
      "INFO:__main__:Processed 112000 issn\n",
      "INFO:__main__:Processed 113000 issn\n",
      "INFO:__main__:Processed 114000 issn\n",
      "INFO:__main__:Processed 115000 issn\n",
      "INFO:__main__:Processed 116000 issn\n",
      "INFO:__main__:Processed 117000 issn\n",
      "INFO:__main__:Processed 118000 issn\n",
      "INFO:__main__:Processed 119000 issn\n",
      "INFO:__main__:Processed 120000 issn\n",
      "INFO:__main__:Processed 121000 issn\n",
      "INFO:__main__:Processed 122000 issn\n",
      "INFO:__main__:Processed 123000 issn\n",
      "INFO:__main__:Processed 124000 issn\n",
      "INFO:__main__:Processed 125000 issn\n",
      "INFO:__main__:Processed 126000 issn\n",
      "INFO:__main__:Processed 127000 issn\n",
      "INFO:__main__:Processed 128000 issn\n",
      "INFO:__main__:Processed 129000 issn\n",
      "INFO:__main__:Processed 130000 issn\n",
      "INFO:__main__:Processed 131000 issn\n",
      "INFO:__main__:Processed 132000 issn\n",
      "INFO:__main__:Processed 133000 issn\n",
      "INFO:__main__:Processed 134000 issn\n",
      "INFO:__main__:Processed 135000 issn\n",
      "INFO:__main__:Processed 136000 issn\n",
      "INFO:__main__:Processed 137000 issn\n",
      "INFO:__main__:Processed 138000 issn\n",
      "INFO:__main__:Processed 139000 issn\n",
      "INFO:__main__:Processed 140000 issn\n",
      "INFO:__main__:Processed 141000 issn\n",
      "INFO:__main__:Processed 142000 issn\n",
      "INFO:__main__:Processed 143000 issn\n",
      "INFO:__main__:Processed 144000 issn\n",
      "INFO:__main__:Processed 145000 issn\n",
      "INFO:__main__:Processed 146000 issn\n",
      "INFO:__main__:Processed 147000 issn\n",
      "INFO:__main__:Processed 148000 issn\n",
      "INFO:__main__:Processed 149000 issn\n",
      "INFO:__main__:Processed 150000 issn\n",
      "INFO:__main__:Processed 151000 issn\n",
      "INFO:__main__:Processed 152000 issn\n",
      "INFO:__main__:Processed 153000 issn\n",
      "INFO:__main__:Processed 154000 issn\n",
      "INFO:__main__:Processed 155000 issn\n",
      "INFO:__main__:Processed 156000 issn\n",
      "INFO:__main__:Processed 157000 issn\n",
      "INFO:__main__:Processed 158000 issn\n",
      "INFO:__main__:Processed 159000 issn\n",
      "INFO:__main__:Processed 160000 issn\n",
      "INFO:__main__:Processed 161000 issn\n",
      "INFO:__main__:Processed 162000 issn\n",
      "INFO:__main__:Processed 163000 issn\n",
      "INFO:__main__:Processed 164000 issn\n",
      "INFO:__main__:Processed 165000 issn\n",
      "INFO:__main__:Processed 166000 issn\n",
      "INFO:__main__:Processed 167000 issn\n",
      "INFO:__main__:Processed 168000 issn\n",
      "INFO:__main__:Processed 169000 issn\n",
      "INFO:__main__:Processed 170000 issn\n",
      "INFO:__main__:Processed 171000 issn\n",
      "INFO:__main__:Processed 172000 issn\n",
      "INFO:__main__:Processed 173000 issn\n",
      "INFO:__main__:Processed 174000 issn\n",
      "INFO:__main__:Processed 175000 issn\n",
      "INFO:__main__:Processed 176000 issn\n",
      "INFO:__main__:Processed 177000 issn\n",
      "INFO:__main__:Processed 178000 issn\n",
      "INFO:__main__:Processed 179000 issn\n",
      "INFO:__main__:Processed 180000 issn\n",
      "INFO:__main__:Saving 180223 results to crossref_journal_metadata.json\n",
      "INFO:__main__:Done\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def install_package(package):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "required_packages = ['backoff']\n",
    "for package in required_packages:\n",
    "    try:\n",
    "        __import__(package)\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        install_package(package)\n",
    "        print(f\"{package} installed successfully!\")\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "from urllib.parse import quote\n",
    "import backoff  # You'll need to pip install backoff\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from typing import Optional, Dict, List\n",
    "import numpy as np\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class CrossRefClient:\n",
    "    \"\"\"Client for making polite requests to CrossRef API\"\"\"\n",
    "    \n",
    "    def __init__(self, email: str, cache_dir: str = \"crossref_cache\"):\n",
    "        self.email = email\n",
    "        self.cache_dir = Path(cache_dir)\n",
    "        self.cache_dir.mkdir(exist_ok=True)\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': f'JournalDataCleaner/1.0 (mailto:{email})'\n",
    "        })\n",
    "        \n",
    "    def _get_cache_path(self, issn: str) -> Path:\n",
    "        \"\"\"Get path for cached response\"\"\"\n",
    "        return self.cache_dir / f\"{issn}.json\"\n",
    "    \n",
    "    def _is_cached(self, issn: str) -> bool:\n",
    "        \"\"\"Check if response is already cached\"\"\"\n",
    "        return self._get_cache_path(issn).exists()\n",
    "    \n",
    "    def _save_to_cache(self, issn: str, data: Dict) -> None:\n",
    "        \"\"\"Save response to cache\"\"\"\n",
    "        with open(self._get_cache_path(issn), 'w') as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "    \n",
    "    def _load_from_cache(self, issn: str) -> Optional[Dict]:\n",
    "        \"\"\"Load response from cache\"\"\"\n",
    "        try:\n",
    "            with open(self._get_cache_path(issn), 'r') as f:\n",
    "                return json.load(f)\n",
    "        except:\n",
    "            return None\n",
    "            \n",
    "    @backoff.on_exception(\n",
    "        backoff.expo,\n",
    "        (requests.exceptions.RequestException, requests.exceptions.HTTPError),\n",
    "        max_tries=8\n",
    "    )\n",
    "    def get_journal_metadata(self, issn: str) -> Optional[Dict]:\n",
    "        \"\"\"\n",
    "        Get journal metadata from CrossRef API with caching and polite waiting\n",
    "        \"\"\"\n",
    "        if not issn or pd.isna(issn):\n",
    "            return None\n",
    "            \n",
    "        # Clean ISSN\n",
    "        issn = str(issn).strip()\n",
    "        if not issn:\n",
    "            return None\n",
    "            \n",
    "        # Check cache first\n",
    "        if self._is_cached(issn):\n",
    "            #logger.info(f\"Loading {issn} from cache\")\n",
    "            return self._load_from_cache(issn)\n",
    "            \n",
    "        # Make request with polite pool\n",
    "        url = f\"https://api.crossref.org/journals/{quote(issn)}\"\n",
    "        params = {'mailto': self.email}\n",
    "        \n",
    "        logger.info(f\"Requesting {issn} from API\")\n",
    "        response = self.session.get(url, params=params)\n",
    "        \n",
    "        # Handle response\n",
    "        if response.status_code == 404:\n",
    "            logger.warning(f\"ISSN {issn} not found\")\n",
    "            return None\n",
    "            \n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        # Cache successful response\n",
    "        self._save_to_cache(issn, data)\n",
    "        \n",
    "        # Polite waiting\n",
    "        time.sleep(1)\n",
    "        \n",
    "        return data\n",
    "\n",
    "def clean_issns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Clean and extract all ISSNs from dataframe\"\"\"\n",
    "    \n",
    "    def extract_additional_issns(val):\n",
    "        if pd.isna(val):\n",
    "            return []\n",
    "        try:\n",
    "            return str(val).split(',')\n",
    "        except:\n",
    "            return []\n",
    "    \n",
    "    # Collect all ISSNs\n",
    "    issns = []\n",
    "    \n",
    "    # Add pissn and eissn\n",
    "    issns.extend(df['pissn'].dropna().unique())\n",
    "    issns.extend(df['eissn'].dropna().unique())\n",
    "    \n",
    "    # Add additional ISSNs\n",
    "    additional = df['additionalIssns'].apply(extract_additional_issns)\n",
    "    issns.extend([issn for sublist in additional for issn in sublist])\n",
    "    \n",
    "    # Clean and deduplicate\n",
    "    issns = [str(issn).strip() for issn in issns if issn and not pd.isna(issn)]\n",
    "    issns = list(set(issns))\n",
    "    \n",
    "    return issns\n",
    "\n",
    "def main():\n",
    "    # Load configuration\n",
    "    EMAIL = \"amy.kirchhoff@ithaka.org\"  # Replace with your email\n",
    "    \n",
    "    # Read CSV file\n",
    "    logger.info(\"Reading CSV file...\")\n",
    "    df = pd.read_csv('CrossRef Data/titleFile.csv', encoding='utf-8')\n",
    "    \n",
    "    # Clean and extract ISSNs\n",
    "    logger.info(\"Extracting ISSNs...\")\n",
    "    issns = clean_issns(df)\n",
    "    logger.info(f\"Found {len(issns)} unique ISSNs\")\n",
    "    \n",
    "    # Initialize client\n",
    "    client = CrossRefClient(EMAIL)\n",
    "\n",
    "\n",
    "    i=0\n",
    "    # Process each ISSN\n",
    "    results = []\n",
    "    for issn in issns:\n",
    "        i+=1\n",
    "        if (i%1000==0):\n",
    "            logger.info(\"Processed \" + str(i) + \" issn\")\n",
    "        try:\n",
    "            metadata = client.get_journal_metadata(issn)\n",
    "            if metadata:\n",
    "                results.append(metadata)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing ISSN {issn}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Save results\n",
    "    output_file = \"crossref_journal_metadata.json\"\n",
    "    logger.info(f\"Saving {len(results)} results to {output_file}\")\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "\n",
    "    logger.info(\"Done\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
